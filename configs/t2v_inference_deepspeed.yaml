TASK_TYPE: inference_t2v_deepspeed
ENABLE: False
use_ema: False
num_workers: 16


max_frames: 16
sample_fps: 3
resolution: [256,256]


# infer_dataset: {
#     'type': 'MSRVTT',
#     'csv_path': 'data/MSRVTT/random_42.csv',
# }

# infer_dataset: {
#     'type': 'EvalCrafter',
#     'prompt_dir_path': 'data/prompts',
# }

# infer_dataset: {
#     'type': 'UCF101ALL',
#     'prompt_path': 'data/UCF101/prompts',
# }

# infer_dataset: {
#     'type': 'UCF101ALL',
#     'prompt_path': 'data/UCF101/prompts_sentence',
# }

infer_dataset: {
    'type': 'VisualDataset',
    'csv_path': 'data/gpt_prompt.csv',
}



embedder: {
    'type': 'FrozenOpenCLIPEmbedder',
    'layer': 'penultimate',
    # 'vit_resolution': [224, 224],
    'pretrained': 'models/modelscope/open_clip_pytorch_model.bin',
    'freeze': True
}

temporal_embedder: {
    'type': 'TemporalEmbedder',
    'layer': 'penultimate',
    # 'vit_resolution': [224, 224],
    'pretrained': 'models/DEMO/temporal_embedder.pth',
    'freeze': True,
    'from_incomplete': True,
}


Pretrain: {
    'type': load_model,
    'from_modelscope': False,
    'inference': True,
    'resume_checkpoint': 'models/DEMO/demo.pth',
}


model_name: "DEMO"

UNet: {
    'type': 'UNetSD_T2V_DEMO',
    'in_dim': 4,
    'y_dim': 1024,
    'upper_len': 128,
    'context_dim': 1024,
    'out_dim': 4,
    'dim_mult': [1, 2, 4, 4],
    'num_heads': 8,
    'default_fps': 8,
    'head_dim': 64,
    'num_res_blocks': 2,
    'dropout': 0.1,
    'misc_dropout': 0.4,
    'temporal_attention': True,
    'temporal_attn_times': 1,
    'use_checkpoint': True,
    'use_fps_condition': False,
    'use_sim_mask': False,
    'text_reweight': True,
}



Diffusion: {
    'type': 'DiffusionDDIM',
    'schedule': 'linear_sd', # cosine
    'schedule_param': {
        'num_timesteps': 1000,
        'init_beta': 0.00085,
        'last_beta': 0.0120,
        'zero_terminal_snr': False,
    },
    'mean_type': 'eps',
    'loss_type': 'mse',
    'var_type': 'fixed_small',
    'rescale_timesteps': False,
    'noise_strength': 0.0
}



guide_scale: 9.0

## vqa encode&decode scale
scale: 8

# Log
log_dir: "inference"


batch_size: 36
chunk_size: 12
decoder_bs: 12

# inference_scale: 8
inference_seed: 42


## from config.py
mean: [0.5, 0.5, 0.5]
std: [0.5, 0.5, 0.5]
max_words: 1000
# num_workers: 8
prefetch_factor: 1
ddim_timesteps: 50  # official: 250
use_div_loss: False

# Model
scale_factor: 0.18215  
# cfg.use_checkpoint = True
# cfg.use_sharded_ddp = False
use_fsdp: False 
use_fp16: True
temporal_attention: True

# cfg.guidances = []

auto_encoder: {
    'type': 'AutoencoderKL',
    'ddconfig': {
        'double_z': True, 
        'z_channels': 4,
        'resolution': 256, 
        'in_channels': 3,
        'out_ch': 3, 
        'ch': 128, 
        'ch_mult': [1, 2, 4, 4],
        'num_res_blocks': 2, 
        'attn_resolutions': [], 
        'dropout': 0.0,
        'video_kernel_size': [3, 1, 1]
    },
    'embed_dim': 4,
    # 'pretrained': 'models/v2-1_512-ema-pruned.ckpt'
    'pretrained': 'models/modelscope/VQGAN_autoencoder.pth'
}

negative_prompt: 'Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms'

# training and optimizer
ema_decay: 0.9999

# lr: 5e-5
weight_decay: 0.0
# betas: (0.9, 0.999)
# eps: 1.0e-8
# chunk_size: 16
# decoder_bs: 8
alpha: 0.7

# scheduler
warmup_steps: 10
decay_mode: 'cosine'


deepspeed_config: "ds_configs/ds_config_inference.json"